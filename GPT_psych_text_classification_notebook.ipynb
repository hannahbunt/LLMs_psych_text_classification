{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4UpZLyz1a4p"
      },
      "source": [
        "# Validating the use of LLMs for psychological text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Python notebook provides the materials to replicate the analysis of the article called \"Validating the use of large language models for psychological text classification\".\n",
        "\n",
        "CONTENTS\n",
        "1. Set up of notebook (installing all the necessary packages from the notebook, setting the API keys for use with OpenAI's GPT). \n",
        "2. Custom functions (defining of any functions necessary for the classifications and subsequent analyses).\n",
        "3. Classifying reported speech (classification 1: extraction of reported speech from diary excerpts).\n",
        "4. Classifying other-initiated repairs (classification 2: binary classification of other-initiated repairs in Reddit dialogues; for the purposes of prompting, these are called 'clarification requests' in the prompt).\n",
        "5. Classifying harm (classification 3: the ordinal classification of harm reported in healthcare complaints submitted to hospitals).\n",
        "\n",
        "In order to run this notebook, you will need the original data. Due to its sensitive nature, this is not publicly available. Yet, it is available upon request. Please email the first author, Hannah Bunt at h.l.bunt@lse.ac.uk "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwJKFrUN5X97"
      },
      "source": [
        "# 1. Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "461aRtD52MHO"
      },
      "outputs": [],
      "source": [
        "# For general processing of data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "#Â For fuzzy matching (reported speech)\n",
        "from thefuzz import fuzz\n",
        "# For plotting:\n",
        "import matplotlib.pyplot as plt\n",
        "# For generating classification reports and accuracy statistics\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "# For progress bars\n",
        "from tqdm import tqdm\n",
        "from IPython.core.display import HTML\n",
        "#For using the OpenAI API and accessing GPT models\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y3JTSIJs5ixO"
      },
      "outputs": [],
      "source": [
        "# Load in openai keys for producing topic model names\n",
        "oai_k = \"\"\n",
        "openai.organization = \"\"\n",
        "openai.api_key = oai_k\n",
        "os.environ['OPENAI_API_KEY'] = oai_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FARLjGM2Lr3"
      },
      "source": [
        "# 2. Custom functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HQWITVe86opP"
      },
      "outputs": [],
      "source": [
        "def get_llm_response(messages, model = \"gpt-4o\", temperature=0, max_tokens = 1500, max_attempts = 3):\n",
        "  '''\n",
        "  Function that takes messages format for ChatGPT input and returns the response text.\n",
        "  '''\n",
        "\n",
        "  for attempt in range(0, max_attempts):\n",
        "    try:\n",
        "      #. request timeout ADD IN\n",
        "      response = openai.chat.completions.create(model=model, messages = messages, temperature=temperature, max_tokens=max_tokens, timeout = 120)\n",
        "      response_text = response.choices[0].message.content\n",
        "      break  # If analysis was successful, break out of the retry loop\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing text on attempt {attempt+1}: {e}\")\n",
        "      if attempt + 1 == max_attempts:\n",
        "        print(f\"Skipping text after {max_attempts} failed attempts.\")\n",
        "        response_text\n",
        "  return response_text\n",
        "\n",
        "def define_messages(prompt, role, text_to_classify):\n",
        "  '''\n",
        "  Function for creating a basic messages format from a prompt, a role, and a text to classify (all strings)\n",
        "  '''\n",
        "  prompt = prompt.format(text_to_classify)\n",
        "  messages = [{'role': 'system', 'content': role},\n",
        "              {'role': 'user', 'content' : prompt}]\n",
        "  return messages\n",
        "\n",
        "def run_classifier(prompt, role, input_texts, model = \"gpt-4o\"):\n",
        "  '''\n",
        "  Function for running a prompt over a series of texts (expects a list)\n",
        "  '''\n",
        "  scores = []\n",
        "  for txt in tqdm(input_texts):\n",
        "    message = define_messages(prompt, role, txt)\n",
        "    try:\n",
        "      response = get_llm_response(message, model)\n",
        "    except:\n",
        "      response = \"Error in response\"\n",
        "    scores.append(response)\n",
        "  return scores\n",
        "\n",
        "def split_string_to_list(string):\n",
        "  '''\n",
        "  Function for splitting a string into a list - used for the reported speech output parsing\n",
        "  '''\n",
        "  regex = '\\s{0,3}\\d{1,3}\\.\\s{1,3}'\n",
        "  list_of_strings = re.split(regex, string)\n",
        "  list_of_strings = [s for s in list_of_strings if len(s)>0]\n",
        "  return list_of_strings\n",
        "\n",
        "def process_RS_scores(llm_scores_RS):\n",
        "  '''\n",
        "  This function correctly parses the llm_scores output into list of strings, removing N/A values.\n",
        "  '''\n",
        "  llm_scores2 = [split_string_to_list(score) for score in llm_scores_RS]\n",
        "  llm_scores3 = []\n",
        "  for list_ in llm_scores2:\n",
        "    out_list = []\n",
        "    for rs in list_:\n",
        "      if rs == \"N/A\":\n",
        "        out_list.append(\"\")\n",
        "      else:\n",
        "        rs = rs.replace('\"','')\n",
        "        out_list.append(rs)\n",
        "    llm_scores3.append(out_list)\n",
        "  return llm_scores3\n",
        "\n",
        "def convert_llm_scores_binary(llm_scores):\n",
        "  '''\n",
        "  Function for converting a string \"Yes\" or \"No\" into binary format - used for the clarification requests\n",
        "  '''\n",
        "  new_scores = []\n",
        "  for s in llm_scores:\n",
        "    if \"yes\" in s.lower():\n",
        "      new_scores.append(1)\n",
        "    else:\n",
        "      new_scores.append(0)\n",
        "  return new_scores\n",
        "\n",
        "def convert_llm_scores_Harm(llm_scores):\n",
        "  '''\n",
        "  Function for converting the Harm scores\n",
        "  '''\n",
        "  new_scores = []\n",
        "  for s in llm_scores:\n",
        "    s = s.lower().replace(' ', '').strip()\n",
        "    if \"noharm\" in s:\n",
        "      new_scores.append(1)\n",
        "    elif \"minimal\" in s:\n",
        "      new_scores.append(2)\n",
        "    elif \"minor\" in s:\n",
        "      new_scores.append(3)\n",
        "    elif \"moderate\" in s:\n",
        "      new_scores.append(4)\n",
        "    elif \"major\" in s:\n",
        "      new_scores.append(5)\n",
        "    elif \"catastrophic\" in s:\n",
        "      new_scores.append(6)\n",
        "    elif \"category1\" in s:\n",
        "      new_scores.append(1)\n",
        "    elif \"category2\" in s:\n",
        "      new_scores.append(2)\n",
        "    elif \"category3\" in s:\n",
        "      new_scores.append(3)\n",
        "    elif \"category4\" in s:\n",
        "      new_scores.append(4)\n",
        "    elif \"category5\" in s:\n",
        "      new_scores.append(5)\n",
        "    elif \"category6\" in s:\n",
        "      new_scores.append(6)\n",
        "    else:\n",
        "      new_scores.append(1)\n",
        "  return new_scores\n",
        "\n",
        "def display_confusion_matrix(human_labels, predicted_labels):\n",
        "  '''\n",
        "  Function for displaying a confusion matrix from results\n",
        "  '''\n",
        "  conf_mx = confusion_matrix(human_labels, predicted_labels)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=conf_mx)\n",
        "  disp.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# functions for fuzzy matching text extraction\n",
        "\n",
        "def get_fuzzScore(human_quote, llm_quote, threshold = 85):\n",
        "  '''\n",
        "  Function for fuzzy matching extracted quotes\n",
        "  '''\n",
        "  if human_quote == llm_quote:\n",
        "    return \"human_positive\", 100\n",
        "  score = fuzz.ratio(human_quote, llm_quote)\n",
        "  if score>threshold:\n",
        "    return \"true_positive\" , score\n",
        "  else:\n",
        "    return \"_unsure_\", score\n",
        "\n",
        "def get_fuzzScoreForList(human_quote, llm_quotes, threshold = 85, debug = False):\n",
        "  '''\n",
        "  Does a list of llm_quotes contain a fuzzy match to the human_quote?\n",
        "  '''\n",
        "  for llm_quote in llm_quotes:\n",
        "    llm_quote = str(llm_quote)\n",
        "    out, score = get_fuzzScore(human_quote, llm_quote, threshold = threshold)\n",
        "    if out == \"true_positive\":\n",
        "      if debug:\n",
        "        bug_print =  f'Fuzzy match = {score}: \\nHuman = \"{human_quote}\"\\nGPT = \"{llm_quote}\"\\n{\"-\"*40}'\n",
        "        return out, bug_print\n",
        "      else:\n",
        "        return out\n",
        "  # if no return by this point\n",
        "  if debug:\n",
        "    return \"_unsure_\", \"\"\n",
        "  return \"_unsure_\"\n",
        "\n",
        "def get_strongMatches(human_quote, llm_quotes, human_noQuoteCode = \"\", llm_noQuoteCode =\"\"):\n",
        "\n",
        "  # make sure quotes have no duplicates\n",
        "  llm_quotes = list(set(llm_quotes))\n",
        "  # if both human and llm find nothing\n",
        "  if len(llm_quotes) == 0:\n",
        "    if str(human_quote)==human_noQuoteCode:\n",
        "        return 'true_negative'\n",
        "    # if human finds nothing but llm finds something\n",
        "    if str(human_quote)==human_noQuoteCode:\n",
        "        return 'false_positive'\n",
        "    # if human finds something but llm finds nothing\n",
        "    if str(human_quote)!=human_noQuoteCode:\n",
        "        return 'false_negative'\n",
        "  elif len(llm_quotes) == 1:\n",
        "    # Check not just list of no quotes\n",
        "    if str(llm_quotes[0]) == llm_noQuoteCode:\n",
        "      if str(human_quote) == human_noQuoteCode:\n",
        "        return \"true_negative\"\n",
        "      else:\n",
        "        return \"false_negative\"\n",
        "    else:\n",
        "      if str(human_quote) == human_noQuoteCode:\n",
        "        return \"false_negative\"\n",
        "      else:\n",
        "        return \"_unsure_\"\n",
        "  else:\n",
        "    return \"_unsure_\"\n",
        "\n",
        "def get_weakMatches(human_quote, llm_quotes, threshold = 85, debug = False):\n",
        "  # WEAKER SUBSET MATCHES\n",
        "  # is one of the quotes a subset of the other?\n",
        "  if len(human_quote)>10: # this prevents a very short string (eg 'a') being a subset - raise to be more cautious\n",
        "    for llm_quote in llm_quotes:\n",
        "      llm_quote = str(llm_quote)\n",
        "      if len(llm_quote)>10: # again, preventing error of very short string being a subset\n",
        "        score = fuzz.partial_ratio(human_quote, llm_quote)\n",
        "        if score>threshold:\n",
        "          if debug:\n",
        "            bug_print = f'Subset match = {score}: \\nHuman = \"{human_quote}\"\\nGPT = \"{llm_quote}\"\\n{\"-\"*40}'\n",
        "            return 'true_positive', bug_print\n",
        "          return 'true_positive'\n",
        "  # if no return by this point\n",
        "  if debug:\n",
        "    return \"false_positive\", \"\"\n",
        "  return \"false_positive\"\n",
        "\n",
        "#Â DEFINE THE MAIN FUNCTION\n",
        "def isQuoteInList(human_quote, llm_quotes, human_noQuoteCode = \"\", llm_noQuoteCode =\"\",\n",
        "                     threshold=85, debug=False):\n",
        "\n",
        "  '''\n",
        "  function to check if the human_quote is in the list of llm_quotes\n",
        "  raise the 'threshold' to be more cautious\n",
        "  '''\n",
        "\n",
        "  # Run strong matches function\n",
        "  accuracy_score = get_strongMatches(human_quote, llm_quotes, human_noQuoteCode, llm_noQuoteCode)\n",
        "  if accuracy_score != \"_unsure_\":\n",
        "    matchStrength = \"strong\"\n",
        "    return accuracy_score, matchStrength\n",
        "\n",
        "  # if no viable score, run fuzzy match function\n",
        "  if debug:\n",
        "    accuracy_score, bug_print = get_fuzzScoreForList(human_quote, llm_quotes, threshold, debug)\n",
        "    print(bug_print)\n",
        "  else:\n",
        "    accuracy_score = get_fuzzScoreForList(human_quote, llm_quotes, threshold, debug)\n",
        "  if accuracy_score != \"_unsure_\":\n",
        "    matchStrength = \"fuzzy\"\n",
        "    return accuracy_score, matchStrength\n",
        "\n",
        "  # if no viable score, run weak match function\n",
        "  if debug:\n",
        "    accuracy_score, bug_print = get_weakMatches(human_quote, llm_quotes, threshold, debug)\n",
        "    print(bug_print)\n",
        "  else:\n",
        "    accuracy_score = get_weakMatches(human_quote, llm_quotes, threshold, debug)\n",
        "  matchStrength = \"subset\"\n",
        "  return accuracy_score, matchStrength\n",
        "\n",
        "# Get classification report and display confusion matrix for reported speech\n",
        "def classification_report_RS(df, scoreColumn):\n",
        "  true_labels, predicted_labels = [],[]\n",
        "  for i, row in df.iterrows():\n",
        "    score = row[scoreColumn]\n",
        "    if score == \"true_positive\":\n",
        "      true_labels.append(1)\n",
        "      predicted_labels.append(1)\n",
        "    elif score == \"true_negative\":\n",
        "      true_labels.append(0)\n",
        "      predicted_labels.append(0)\n",
        "    elif score == \"false_positive\":\n",
        "      true_labels.append(0)\n",
        "      predicted_labels.append(1)\n",
        "    elif score == \"false_negative\":\n",
        "      true_labels.append(1)\n",
        "      predicted_labels.append(0)\n",
        "  print(classification_report(true_labels, predicted_labels))\n",
        "  display_confusion_matrix(true_labels, predicted_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Classifying reported speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gO6ZHmMQ-asm"
      },
      "outputs": [],
      "source": [
        "role_RS = \"You are a helpful research assistant that identifies reported speech.\"\n",
        "\n",
        "min_definition_RS =\"\"\"\n",
        "DEFINITION:\n",
        "Reported speech can be direct or indirect.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "max_definition_RS = \"\"\"\n",
        "DEFINITION:\n",
        "I want to teach you how to do a 'reported speech analysis' on diary entries. Reported speech is how we represent the speech of other people or what we ourselves say. There are two main types of reported speech: direct speech and indirect speech.\n",
        "Direct speech repeats the exact words the person used, or how we remember their words.\n",
        "Indirect speech reports the original speakerâs words are changed. Indirect speech focuses more on the content of what someone said rather than their exact words.\n",
        "Speech reports consist of two parts: the reporting clause and the reported clause. The reporting clause includes a verb such as 'say', 'tell', 'ask', 'reply', 'shout', 'assert', 'whisper', 'write', 'type'. The reported clause includes what the original speaker said.\n",
        "Sometimes, objects are metaphorically used when someone says or has said something. This is metonymy and should also be considered 'reported speech'.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "examples_RS = \"\"\"\n",
        "EXAMPLES:\n",
        "Example of direct speech: âIâm sorry,â said Mark.\n",
        "Example of indirect speech: Mark apologised.\n",
        "Example of direct speech: Barbara said, âI didnât realise it was midnight.â\n",
        "Example of indirect speech: Barbara said she hadnât realised it was midnight.\n",
        "Example of metonymy: I was contacted by the appliance store, the blower wheel had been delivered.\n",
        "Explanation: Someone from the appliance store called the narrator saying that the blower wheel had been delivered.\n",
        "Example of metonymy: Pecans are in great demand this year according to a NPR story.\n",
        "Explanation: The NPR story, even though is not a person, reports that pecans are in great demand this year.\n",
        "Example of metonymy: A private note asked for my perspective on certain aspects of the gay community.\n",
        "Explanation: The private note 'asked' for the narrator's perspective on certain aspects of the gay community.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "suffix_RS = \"\"\"\n",
        "INSTRUCTION:\n",
        "Is there reported speech in the diary entry? If there is reported speech, provide the exact quote from the text containing the reporting clause and reported clause.\n",
        "Include all the instances of reported speech and number them.\n",
        "Only respond with the instances of reported speech.\n",
        "If there is no reported speech in the input text, return 'N/A'.\n",
        "\n",
        "DIARY ENTRY:\n",
        "```{}```\n",
        "\"\"\"\n",
        "\n",
        "prompt_RS_minZero = \"\\n\".join([min_definition_RS, suffix_RS])\n",
        "prompt_RS_maxZero = \"\\n\".join([max_definition_RS, suffix_RS])\n",
        "prompt_RS_maxFew = \"\\n\".join([max_definition_RS, examples_RS, suffix_RS])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9_uMVX6-2iS"
      },
      "source": [
        "## Load data\n",
        "\n",
        "Data available upon request. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KcsH2EPgKVUQ"
      },
      "outputs": [],
      "source": [
        "# Convert input texts to a list\n",
        "input_texts_RS = df_RS[\"Text\"].to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlq3FQ1B_HgZ"
      },
      "source": [
        "## Extract reported speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtDPHLzuKmLI",
        "outputId": "24f86e19-4c2d-452a-b2ec-f4151b273b0c"
      },
      "outputs": [],
      "source": [
        "llm_scores_RS = run_classifier(prompt_RS_maxFew, role_RS, input_texts_RS, model = \"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_scores_RS2 = process_RS_scores(llm_scores_RS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EOfoVh5j_KHH"
      },
      "outputs": [],
      "source": [
        "# Process llm_scores output - makes into list and cleans up the strings\n",
        "df_RS['llm_hits'] = llm_scores_RS2\n",
        "# split strings from Manual_span\n",
        "df_RS['human'] = df_RS['Manual_span'].apply(split_string_to_list)\n",
        "# explode the human quotes - so there is one true human quote per row\n",
        "df_RS_expl = df_RS.explode('human').reset_index(drop=True)\n",
        "# Replace the values '-' with empty value\n",
        "df_RS_expl = df_RS_expl.replace('-', \"\")\n",
        "# categorise the matches\n",
        "results_RS = df_RS_expl.apply(lambda x: isQuoteInList(x['human'], x['llm_hits']), axis=1)\n",
        "df_RS_expl[['llm_match', 'matchStrength']] = pd.DataFrame(results_RS.tolist(), index=df_RS_expl.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnOazEfT_VBg"
      },
      "source": [
        "## Predictive validity test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# True/False - has reported speech\n",
        "def decision_function(x, cutoff = 6):\n",
        "  if x == \"['']\":\n",
        "    return False\n",
        "  elif x == \"['-']\":\n",
        "    return False\n",
        "  elif len(x) <= cutoff:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "llm_trueFalse_RS = [decision_function(x) for x in df_RS[\"llm_hits\"].astype(str).to_list() ]\n",
        "human_trueFalse_RS = [decision_function(x) for x in df_RS[\"human\"].astype(str).to_list() ]\n",
        "\n",
        "\n",
        "print(classification_report(human_trueFalse_RS, llm_trueFalse_RS))\n",
        "display_confusion_matrix(human_trueFalse_RS, llm_trueFalse_RS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Qualitative content validity checks\n",
        "Table below is built on exploded dataframe; the confusion matrix above uses the original dataframe. As a result, there might be more false positives and false negatives in the tables below than in the confusion matrix. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_RS_out = df_RS_expl[ ['Text', 'human', 'llm_hits', 'llm_match']].copy()\n",
        "df_RS_out['llm_hits'] = df_RS_out['llm_hits'].apply(lambda x: '<br><br>'.join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# investigate false negatives\n",
        "df_fp = df_RS_out[df_RS_out['llm_match']=='false_negative'].copy()\n",
        "# add title\n",
        "df_fp = df_fp.style.set_caption(f\"Reported Speech: False Negatives (n={len(df_fp)})\")\n",
        "# display\n",
        "with pd.option_context('display.max_colwidth', 0):\n",
        "    display(HTML(df_fp.to_html(escape=False)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# investigate false positives\n",
        "df_fn = df_RS_out[df_RS_out['llm_match']=='false_positive'].copy()\n",
        "# add title\n",
        "df_fn = df_fn.style.set_caption(f\"Reported Speech: False Positives (n={len(df_fn)})\")\n",
        "# display\n",
        "with pd.option_context('display.max_colwidth', 0):\n",
        "    display(HTML(df_fn.to_html(escape=False)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ8bMWbn88jD"
      },
      "source": [
        "# 4. Classifying other-initiated repairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VwjJ9vqF2MeG"
      },
      "outputs": [],
      "source": [
        "role_CR = \"\"\"\n",
        "You are a helpful research assistant that identifies clarification requests.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "min_definition_CR = \"\"\"\n",
        "DEFINITION:\n",
        "Clarification requests are questions or statements designed to address problems of miscommunication and misunderstanding.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "max_definition_CR = \"\"\"\n",
        "DEFINITION:\n",
        "Clarification requests occur in response to unclear or ambiguous communication and aim to enhance comprehension and avoid misunderstandings.\n",
        "They are essentially questions that help individuals gather more information or confirm their assumptions, promoting precise understanding and reducing the risk of misinterpretation.\n",
        "There are four types of clarification requests:\n",
        "Type 1: Direct expressions of confusion, where the individual seeks a repetition or rephrasing of the original statement.\n",
        "Type 2: Targeted inquiries aimed at extracting specific elements or details within the provided information.\n",
        "Type 3: Proposing a suggestion or hypothesis to confirm understanding, essentially narrowing down possibilities to affirm comprehension.\n",
        "Type 4: A rhetorical question (of types 1-3) that can be answered as if it were a clarification request.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "examples_CR = \"\"\"\n",
        "EXAMPLES\n",
        "----\n",
        "SPEAKER A:\n",
        "\"I think Iâve seen this plotline somewhere beforeâ\n",
        "\n",
        "SPEAKER B:\n",
        "\"Are you thinking of 'Cryptonomicon' by Neal Stephenson? The main character cracks codes during World War II and there's this intricate connection to modern cryptography and technology. I remember reading it a while back and being hooked.\"\n",
        "\n",
        "Expected response:\n",
        "YES\n",
        "----\n",
        "SPEAKER A:\n",
        "âYeah this really makes me think of the Star Wars robotâ\n",
        "\n",
        "SPEAKER B:\n",
        "\"R2-D2 or C-3PO?\"\n",
        "\n",
        "Expected response:\n",
        "YES\n",
        "----\n",
        "SPEAKER A:\n",
        "\"Title: Marvel's Multiverse Mayhem Teaser Body: \"\n",
        "\n",
        "SPEAKER B:\n",
        "\"Did you also catch the new DC multiverse trailer? I'm honestly on the fence about it. The multiverse concept is intriguing, but I'm curious to hear what others think. Is it living up to the hype or falling flat for you?\"\n",
        "\n",
        "Expected response:\n",
        "YES\n",
        "----\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "instruction_CR = \"\"\"\n",
        "INSTRUCTION:\n",
        "In the interaction below, does speaker B use a clarification request in their response to speaker A?\n",
        "Respond with 'YES' if there is a clarification request. Respond with 'NO' if not.\n",
        "Answer 'YES' if unsure.\n",
        "Answer 'YES' if the clarification request is rhetorical.\n",
        "\n",
        "INTERACTION:\n",
        "```{}```\n",
        "Expected response:\n",
        "\"\"\"\n",
        "\n",
        "prompt_minZero_CR = \"\\n\".join([min_definition_CR, instruction_CR])\n",
        "prompt_maxZero_CR = \"\\n\".join([max_definition_CR, instruction_CR])\n",
        "prompt_maxFew_CR = \"\\n\".join([max_definition_CR, examples_CR, instruction_CR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data\n",
        "\n",
        "Data is available upon request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "gQeTZezx9Xom"
      },
      "outputs": [],
      "source": [
        "# Create the input text lists for the classifier\n",
        "input_texts_CR = CR_0[\"text\"].to_list() \n",
        "true_labels_CR = CR_0[\"labels\"].to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2ov5ovi-AcH"
      },
      "source": [
        "## Classify the interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMcH42ci9tz8",
        "outputId": "2c655654-3e71-461a-8d3a-32d78544aba3"
      },
      "outputs": [],
      "source": [
        "# Choose a prompt type and get the GPT scores\n",
        "llm_scores_CR = run_classifier(prompt_maxFew_CR, role_CR, input_texts_CR, model = \"gpt-4o\")\n",
        "# Convert GPT scores to binary\n",
        "llm_binary_CR = convert_llm_scores_binary(llm_scores_CR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acm1h9nQ-M7d"
      },
      "source": [
        "## Predictive validity test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "XbWqodI-SwmZ",
        "outputId": "354652fd-f167-49ec-cd3b-02e9845fea82"
      },
      "outputs": [],
      "source": [
        "print(classification_report(true_labels_CR, llm_binary_CR))\n",
        "display_confusion_matrix(true_labels_CR, llm_binary_CR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Qualitative content validity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "texts_CR = [x.replace('\\n', '<br>') for x in input_texts_CR]\n",
        "human_labels_CR = [bool(x) for x in true_labels_CR]\n",
        "llm_labels_CR = [bool(x) for x in llm_binary_CR]\n",
        "df_CR = pd.DataFrame({'texts': texts_CR, 'human_labels': human_labels_CR, 'llm_labels': llm_labels_CR})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# examine false negatives\n",
        "with pd.option_context('display.max_colwidth', 0):\n",
        "    # select rows\n",
        "    df_fn = df_CR[(df_CR['human_labels']==True)&(df_CR['llm_labels']==False)]\n",
        "    # add title\n",
        "    df_fn = df_fn.style.set_caption(f\"Clarification Requests: False Negatives (Human=True, LLM=False, n={len(df_fn)})\")\n",
        "    # display as html with line breaks\n",
        "    display(HTML(df_fn.to_html(escape=False)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# examine false positives\n",
        "with pd.option_context('display.max_colwidth', 0):\n",
        "    # select rows\n",
        "    df_fp = df_CR[(df_CR['human_labels']==False)&(df_CR['llm_labels']==True)]\n",
        "    # add title\n",
        "    df_fp = df_fp.style.set_caption(f\"Clarification Requests: False Positives. (Human=False, LLM=True, n={len(df_fp)})\")\n",
        "    # display as html with line breaks\n",
        "    display(HTML(df_fp.to_html(escape=False)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Classifying harm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complaint summary prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "role_Summary = \"You are an AI designed to clean text data.\"\n",
        "\n",
        "prompt_Summary = \"\"\"\n",
        "INSTRUCTIONS:\n",
        "Below is a hospital complaint.  Any identifiable information has been redacted. As a result, the complaints are difficult to read. \n",
        "Please: (1) fill in the gaps in the text with the most appropriate named entity placeholders, (2) clean the text so that it is legible, and (3) return only new text. \n",
        "COMPLAINT:\n",
        "```{}```\n",
        "NEW COMPLAINT:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "role_Harm = \"Your role is to methodically evaluate the level of severity in patient complaints concerning healthcare services. Your assessment should be based solely on the direct outcomes of accidents or negligence by healthcare staff, impacting the patient's health and daily life.\"\n",
        "\n",
        "min_definition_Harm = \"\"\"\n",
        "DEFINITION:\n",
        "Severity in healthcare incidents is measured by the direct negative impact on a patient's health and daily activities, originating solely from healthcare staff's accidents or negligence. \n",
        "This ranges from no impact ('Category 1') to severe, life-altering impacts ('Category 6').\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "max_definition_Harm = \"\"\"\n",
        "DEFINITION:\n",
        "Severity in healthcare incidents is measured by the direct negative impact on a patient's health and daily activities, originating solely from healthcare staff's accidents or negligence. \n",
        "This ranges from no impact ('Category 1') to severe, life-altering impacts ('Category 6').\n",
        "\n",
        "Severity Levels:\n",
        "Category 1: No impact - no treatment or intervention needed.\n",
        "Category 2: Minimal impact - minor treatment, no work absence.\n",
        "Category 3: Minor impact - some treatment, <3 days off work, <4 days extra hospital stay.\n",
        "Category 4: Moderate impact - significant treatment, 4-14 days off work, 4-15 days extra hospital stay, reportable incidents.\n",
        "Category 5: Major impact - long-term health consequences, >14 days off work, >15 days extra hospital stay.\n",
        "Category 6: Catastrophic impact - death, irreversible harm, large-scale patient impact.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "examples_Harm = \"\"\"\n",
        "EXAMPLES:\n",
        "Category 1: No reported health impact.\n",
        "Category 2: Non-consumed medication errors, minor injuries like bruises.\n",
        "Category 3: Non-harmful medication errors, minor conditions like grade 1 pressure ulcers, minor physical or mental health issues without work absence.\n",
        "Category 4: Harmful medication errors, moderate conditions like grade 2/3 pressure ulcers, infections, misinformation impacting patient transfer, injuries requiring some medical attention.\n",
        "Category 5: Serious medication errors with severe effects, grade 4 pressure ulcers, long-term infections, surgical errors requiring additional surgery, serious injuries like loss of a limb.\n",
        "Category 6: Fatal incidents, suicide, severe surgical errors like wrongful amputation, paralysis, serious assaults.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "instructions_Harm = \"\"\"\n",
        "INSTRUCTIONS:\n",
        "Analyse a healthcare service complaint using these steps:\n",
        "Step 1: Read the patient complaint.\n",
        "Step 2: Evaluate the severity of direct negative impacts on the patient's health and daily life caused solely by hospital staff. Consider only the facts presented in the complaint.\n",
        "Step 3: Categorize the severity level using the provided six categories. Only clear and unambiguous evidence should lead to categorizing an incident. \n",
        "Step 4: Respond only with the relevant severity category (e.g., 'Category 1', 'Category 2', etc.).\n",
        "\n",
        "COMPLAINT:\n",
        "```{}```\n",
        "CATEGORY:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt_Harm_minZero = \"\\n\".join([min_definition_Harm, instructions_Harm])\n",
        "prompt_Harm_maxZero = \"\\n\".join([max_definition_Harm, instructions_Harm])\n",
        "prompt_Harm_maxFew = \"\\n\".join([max_definition_Harm, examples_Harm, instructions_Harm])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load harm data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "C-xu6nct_ZiQ"
      },
      "outputs": [],
      "source": [
        "# convert Harm to number\n",
        "Harm_dic = {'NoHarm':1, 'Minimal':2, 'Minor':3, 'Moderate':4, 'Major':5, 'Catastrophic':6}\n",
        "df_Harm0['Harm_num'] = df_Harm0['Harm'].replace(Harm_dic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summarise harm data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Â Summarising harm:\n",
        "input_texts_Harm = df_Harm[\"Text\"].to_list()\n",
        "df_Harm[\"summaries\"] = run_classifier(prompt_Summary, role_Summary, input_texts_Harm, model = \"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq0Cvlhq_8JG"
      },
      "source": [
        "## Classifying harm with LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0-oMyA7Bls0",
        "outputId": "3566c67a-7820-4ac4-9660-b2d025066185"
      },
      "outputs": [],
      "source": [
        "input_texts_Harm = df_Harm[\"summaries\"].to_list() \n",
        "llm_scores_Harm = run_classifier(prompt_Harm_maxFew, role_Harm, input_texts_Harm, model = \"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add the new columns to the existing df DataFrame\n",
        "df_Harm['llm_harm'] = llm_scores_Harm\n",
        "df_Harm['llm_harm_num'] = convert_llm_scores_Harm(llm_scores_Harm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yhps1dlAO_d"
      },
      "source": [
        "## Predictive validity test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "human_harm = df_Harm['Harm_num'].to_list()\n",
        "llm_harm = df_Harm['llm_harm_num'].to_list()\n",
        "percent_agreement = sum([1 for i, j in zip(human_harm, llm_harm) if i == j])/len(human_harm)\n",
        "print(f'Percent agreement: {percent_agreement:.2f}')\n",
        "kappa = cohen_kappa_score(human_harm, llm_harm, weights='quadratic')\n",
        "print(f'Cohens Weighted Kappa for ordinal classifications: {kappa:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Qualitative content validity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_Harm2 = df_Harm.copy()\n",
        "df_Harm2['discrepancy'] = df_Harm2['Harm_num'] - df_Harm2['llm_harm_num']\n",
        "df_Harm2 = df_Harm2.drop(columns=['Harm_num', 'llm_harm_num'])\n",
        "df_Harm2['llm_harm'] = df_Harm2['llm_harm'].str.replace('\"', '').str.strip()\n",
        "df_Harm2['Text'] = df_Harm2['Text'].str.replace('\\n', ' ----- ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# investigate false negatives\n",
        "df_fp = df_Harm2[df_Harm2['Harm']!=df_Harm2['llm_harm']].copy()\n",
        "# sort by discrepancy\n",
        "df_fp = df_fp.sort_values(by=['discrepancy'], ascending=False)\n",
        "# add title\n",
        "df_fp = df_fp.style.set_caption(f\"Harm: Disagreements (n={len(df_Harm2)})\")\n",
        "# display\n",
        "with pd.option_context('display.max_colwidth', 0):\n",
        "    display(df_fp)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
